{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE4211 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Team 1: Quadratic                   Done by Choi Jae Hyung, Lee Min Young, Nicholas Lui Ming Yang, Tran Duy Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"project_data.csv\")\n",
    "# df.shape - (1584823, 3)\n",
    "# df column values:\n",
    "# localminute - Timestamp \n",
    "# dataid - MeterID \n",
    "# meter_value - meter reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many houses are included in the measurement study?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since dataid is unique for each meter, we can count the number of unique dataid numbers\n",
    "df.dataid.value_counts().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This gives us 157 for the number of houses included in the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are there any malfunctioning meters? If so, identify them and the time periods where they were malfunctioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are various ways to define a malfunctioning meter. Let us explore some of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us first check, if there are *meters that report a lower value at a later timestamp*. Since meter_value is cumulative consumption, meter_value should not be lower at a later timestamp for the same meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(['dataid'], sort=['localminute'])\n",
    "def has_decreasing_values(df):\n",
    "    current_value = 0\n",
    "    for index, val in df.iteritems():\n",
    "        if val < current_value:\n",
    "            return True\n",
    "        current_value = val\n",
    "        \n",
    "meters_with_decreasing = (grouped['meter_value']\n",
    "                          .agg(has_decreasing_values)\n",
    "                          .where(lambda x: x)\n",
    "                          .dropna()\n",
    "                          .keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([  35,   77,   94,  483,  484, 1042, 1086, 1185, 1507, 1556, 1718,\n",
       "            1790, 1801, 2129, 2335, 2449, 3134, 3527, 3544, 3893, 4031, 4193,\n",
       "            4514, 4998, 5129, 5131, 5193, 5403, 5810, 5814, 5892, 6836, 7017,\n",
       "            7030, 7117, 7739, 7794, 7989, 8156, 8890, 9134, 9639, 9982],\n",
       "           dtype='int64', name='dataid')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(meters_with_decreasing))\n",
    "meters_with_decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wow, we have 43 meters that have a decreasing value! Let's zoom in and get the offending sections for each meter. (In other words, find the exact data points which show this descreasing value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5b4b695c3543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meter_value'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcurrent_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0moffending_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcurrent_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meter_value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1326\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1328\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_loc\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   1897\u001b[0m                                                       \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m                                                       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1899\u001b[1;33m                                                       dtype=new_values.dtype)\n\u001b[0m\u001b[0;32m   1900\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_validate_dtype\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;31m# a compound dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Need to re-sort after filter since filter takes all rows and breaks original sorting\n",
    "dec_meters = (grouped.filter(lambda x: int(x.name) in meters_with_decreasing)\n",
    "              .groupby(['dataid'], sort=['localminute']))\n",
    "\n",
    "## Iterate over values to find offending rows for each meter\n",
    "## WARNING: RUNS VERY SLOWLY. TODO: OPTIMIZE\n",
    "offending_values = {}\n",
    "for group_id, rows in dec_meters.groups.items():\n",
    "    offending_values[group_id] = []\n",
    "    current_value = 0\n",
    "    group_rows = dec_meters.get_group(group_id)\n",
    "    group_rows_count = group_rows.shape[0]\n",
    "    for i in range(group_rows_count):\n",
    "        if group_rows.iloc[i]['meter_value'] < current_value:\n",
    "            offending_values[group_id].append([group_rows.iloc[i-1], group_rows.iloc[i]])\n",
    "        current_value = group_rows.iloc[i]['meter_value']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of 'broken data' instances for each meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meter ID |\", \"Number of broken readings\")\n",
    "for k, v in offending_values.items():\n",
    "    print(str(k).ljust(20), len(v))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just knowing the number of broken readings may not be useful. \n",
    "##### Let's say that we want to know which meters should be fixed, since faulty meters result in us inaccurately measuring consumption, and possibly losing money.\n",
    "##### There should be some measure of tolerance in deciding if a meter is broken. In this case, let's check the average decreasing amount, and the ratio of \"broken\" readings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Meter ID |\", \"Number of broken readings |\", \"Average decrease across broken readings |\", \"Percentage of broken readings for meter\")\n",
    "for k, v in offending_values.items():\n",
    "    print(str(k).ljust(20), \n",
    "          str(len(v)).ljust(20), \n",
    "          str(reduce(lambda x, y: x + abs(y[1]['meter_value'] - y[0]['meter_value']) / len(v), [0] + v )).ljust(40), \n",
    "          (len(v) / dec_meters.get_group(k).shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the data laid out like this, it is pretty clear that most meters are not really broken if we allow a 2% error rate\n",
    "#### However, in terms of error volume, some are pretty suspect. Let's use an average volume error of 100 Cubic foot (that's a lot!) as our threshold, and filter our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meter ID |\", \"Number of broken readings |\", \"Average decrease across broken readings |\", \"Percentage of broken readings for meter\")\n",
    "for k, v in offending_values.items():\n",
    "    measure = str(reduce(lambda x, y: x + abs(y[1]['meter_value'] - y[0]['meter_value']) / len(v), [0] + v )).ljust(40)\n",
    "    if float(measure) > 10:\n",
    "        print(str(k).ljust(20), \n",
    "              str(len(v)).ljust(20), \n",
    "              measure, \n",
    "              (len(v) / dec_meters.get_group(k).shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use this function to validate/check each bad meter given our heuristic\"\"\"\n",
    "def print_bad_meter_readings(meterID):\n",
    "    meter_readings = offending_values[meterID]\n",
    "    print(\"time apart |\".ljust(20), \"meter value initial |\", \"meter value after |\", \"difference\")\n",
    "    for readings_pair in meter_readings:\n",
    "\n",
    "        print(str(pd.to_datetime(readings_pair[1]['localminute']) - pd.to_datetime(readings_pair[0]['localminute'])).ljust(25), \n",
    "              str(readings_pair[0]['meter_value']).ljust(20), \n",
    "              str(readings_pair[1]['meter_value']).ljust(20), \n",
    "              readings_pair[1]['meter_value'] - readings_pair[0]['meter_value'])\n",
    "print_bad_meter_readings(1556)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another requirement of the meters is that they push their readings to be saved if the meter values differ by at least 2 cubic foot\n",
    "#### Let us verify that every pair of readings for a meter differ by at least 2 cubic foot. To not double count, we will only check for readings where the differing value is 0 >= x > 2 so that we don't get the same error readings where the readings decrease\n",
    "#### Since the value is smaller, we will not focus on the difference in values, but on the percentage of total readings for the meter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2 = df.groupby(['dataid'], sort=['localminute'])\n",
    "def has_stagnant_values(df):\n",
    "    current_value = 0\n",
    "    for index, val in df.iteritems():\n",
    "        if index == 0:\n",
    "            current_value = val\n",
    "            continue\n",
    "        \n",
    "        if val < current_value + 2 and val >= current_value:\n",
    "            return True\n",
    "        current_value = val\n",
    "        \n",
    "meters_with_stagnant = (grouped['meter_value']\n",
    "                          .agg(has_stagnant_values)\n",
    "                          .where(lambda x: x)\n",
    "                          .dropna()\n",
    "                          .keys())\n",
    "\n",
    "print(\"Number of meters with stagnant values: \", len(meters_with_stagnant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following segment should have calculated the percentage of stagnant values for each meter, but it takes too long (tried for 5 minutes and gave up). At least we know number of meters that reported stagnant values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Need to re-sort after filter since filter takes all rows and breaks original sorting\n",
    "# stagnant_meters = (grouped2.filter(lambda x: int(x.name) in meters_with_stagnant)\n",
    "#               .groupby(['dataid'], sort=['localminute']))\n",
    "\n",
    "# ## Iterate over values to count offending occurences. Not counting value\n",
    "# ## WARNING: RUNS VERY SLOWLY. TODO: OPTIMIZE\n",
    "# offending_values2 = {}\n",
    "# for group_id, rows in stagnant_meters.groups.items():\n",
    "#     offending_values2[group_id] = 0\n",
    "#     group_rows = stagnant_meters.get_group(group_id)\n",
    "#     group_rows_count = group_rows.shape[0]\n",
    "#     # Set current_value so we do not trigger first reading\n",
    "#     current_value = group_rows.iloc[0]['meter_value'] - 5\n",
    "#     for i in range(group_rows_count):\n",
    "#         if group_rows.iloc[i]['meter_value'] < current_value + 2 and group_rows.iloc[i]['meter_value'] >= current_value:\n",
    "#             offending_values2[group_id] += 1\n",
    "#         current_value = group_rows.iloc[i]['meter_value']\n",
    "\n",
    "        \n",
    "# print(\"Meter ID |\", \"Number of broken readings |\", \"Percentage of broken readings for meter\")\n",
    "# for k, v in offending_values2.items():\n",
    "#     print(str(k).ljust(20), \n",
    "#           str(v).ljust(20),  \n",
    "#           (v / stagnant_meters.get_group(k).shape[0]) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagnant_meters = (grouped2.filter(lambda x: int(x.name) in meters_with_stagnant)\n",
    "              .groupby(['dataid'], sort=['localminute']))\n",
    "\n",
    "print(\"Total number of readings to iterate, which is why it is slow: \", \n",
    "      reduce(lambda x, y: x + len(y), [0] + list(stagnant_meters.groups.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have two different criterion for deciding what constitues a broken meter, and the readings that helped us determine that. Since the question asks for \"time periods where they were malfunctioning.\", let's demonstrate that we can consolidate broken readings into time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's use the meter with the most broken readings (based off decreasing values)\n",
    "most_broken_values_meter = grouped.get_group(5403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are using a stricter requirement now. Let's consider that, if the subsequent meter reading is less \n",
    "# than an increment of 2 (meters are only supposed to report after at least an increment of 2)\n",
    "\n",
    "# Broken_criteria is a helper function (comparator function) that takes in two (in-order) readings and outputs a \n",
    "# boolean of whether the later reading is \"broken\"\n",
    "\n",
    "broken_criteria = lambda x, y: y['meter_value'] < x['meter_value'] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_readings = 0\n",
    "num_readings = most_broken_values_meter.shape[0]\n",
    "for i in range(1, num_readings):\n",
    "    if broken_criteria(most_broken_values_meter.iloc[i-1], most_broken_values_meter.iloc[i]):\n",
    "        broken_readings += 1\n",
    "\n",
    "print(\"Number of broken readings for meter 5403, based on stricter requirements: \", broken_readings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's now create a function that can aggregate a meter's readings into \"broken\" time periods\n",
    "# Return value of the function will be as follow:\n",
    "\"\"\"\n",
    "2D array. \n",
    "First dimension is the time periods. \n",
    "Second dimension is the consecutive broken readings that make up the broken time period\n",
    "To find the actual time data, take the ['localminute'] attribute from the first and last reading per period\n",
    "    time_periods = [\n",
    "        [reading_1, reading_2, reading_3],\n",
    "        [reading_1, reading_2]\n",
    "    ]\n",
    "\"\"\"\n",
    "def get_broken_time_periods(broken_criteria, meter_readings):\n",
    "    num_readings = meter_readings.shape[0]\n",
    "    time_periods = []\n",
    "    temp_period = []\n",
    "    for i in range(1, num_readings):\n",
    "        if broken_criteria(meter_readings.iloc[i-1], meter_readings.iloc[i]):\n",
    "            temp_period.append(meter_readings.iloc[i])\n",
    "        else:\n",
    "            if temp_period:\n",
    "                time_periods.append(temp_period)\n",
    "                temp_period = []\n",
    "    if temp_period:\n",
    "        time_periods.append(temp_period)\n",
    "    return time_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_time_periods = get_broken_time_periods(broken_criteria, most_broken_values_meter)\n",
    "print(\"Number of broken time periods: \", len(broken_time_periods))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.2 \n",
    "## Hourly Data Collection and Plotting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: \n",
    "Obtain hourly data from a list of given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters: \n",
    "#### 1. Gas meter ID \n",
    "Gas meter with ID 739 will be focused for the study\n",
    "#### 2. Starting time\n",
    "1st Oct 2015, 6 a.m.\n",
    "#### 3. Ending time\n",
    "1st Nov 2015, 6 a.m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumption:\n",
    "1. Starting time is within the given data range.\n",
    "2. The data is cumulative: either constant or increasing. Hence, the hourly data is obtained from the most recent data from the previous hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to handle missing data?\n",
    "When there is a period of missing data, there may not be most recent data present close to an exact hour.\n",
    "As the data is cumulative gas consumption by household, when there is no data, it can be assumed as there is insignificant gas consumption between that period.\n",
    "\n",
    "Hence, the missing data will be given same value as most recent hourly data obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm of hourly data collection consists of the functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When the hour hand changes, collect hourly data from the previous data.\n",
    "\n",
    "2. Look out for missing hour. When missing hour is found, previuos hour data is given for that hour.\n",
    "\n",
    "3. When starting hour is exact hour (hh:00:00), hourly data begins to be collected from next hour.\n",
    "\n",
    "When a list of data is provided, obtaining hourly data from the starting exact hour may lead to over-estimation as there could possibly be a lot of consumption between the exact hour and the next data found after that hour.\n",
    "\n",
    "When ending hour is hh:00:00, hourly data of hh:00:00 will be the last hourly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hourly data collected\n",
    "hourly_meter = []\n",
    "\n",
    "#period_start format: 'yyyy-mm-dd hh:mm:ss'\n",
    "def start_period(period_start):\n",
    "    start_datetime = pd.to_datetime(period_start)\n",
    "    return start_datetime\n",
    "\n",
    "#period_end format: 'yyyy-mm-dd hh:mm:ss'\n",
    "def end_period(period_end):\n",
    "    end_datetime = pd.to_datetime(period_end)\n",
    "    return end_datetime                \n",
    "    \n",
    "def hourly_reading():\n",
    "       \n",
    "    hourly_meter.clear()\n",
    "    first_hour = t.dt.hour[0]\n",
    "    \n",
    "    for dataframe_row, value in t.dt.hour.iteritems():\n",
    "        \n",
    "        if (dataframe_row != 0):\n",
    "            \n",
    "            # Smallest difference is 28 days in Feb - First day of the month. Hence 28 - 1 = 27\n",
    "            if ((t[dataframe_row].day - t[dataframe_row - 1].day) >= 27):\n",
    "                print (\"There is missing data while the month is changed. Please check.\")\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                if (t.dt.hour[dataframe_row] != 0):\n",
    "                    if (t[dataframe_row].day != t[dataframe_row - 1].day):\n",
    "                        if ((t[dataframe_row].hour - t[dataframe_row - 1].hour) >= 0):\n",
    "                            for i in range (0, 24*(t[dataframe_row].day - t[dataframe_row - 1].day)):\n",
    "                                hourly_meter.append(temp.meter_value[dataframe_row-1])\n",
    "\n",
    "                        else:\n",
    "                            # e.g. final hour is day 2 00:00:00, initial hour is day 1 20:00:00,  there is 4 hours gap, not 28 hours.\n",
    "                            if ((t[dataframe_row].day - t[dataframe_row - 1].day) > 1):\n",
    "                                for i in range (0, 24*(t[dataframe_row].day - t[dataframe_row - 1].day - 1)):\n",
    "                                    hourly_meter.append(temp.meter_value[dataframe_row-1])\n",
    "\n",
    "                            else:\n",
    "                                return\n",
    "                \n",
    "        if (t.dt.hour[dataframe_row] == first_hour):\n",
    "                        \n",
    "            # Taking note of jump in data \n",
    "            # If there is data that is hh:00:00 and this is starting point, data will be added as the first hourly value.\n",
    "            # Else, the first hourly value will be the next hour.\n",
    "            if (dataframe_row == 0):\n",
    "                if ((t[0].minute == 0) and (t[0].second == 0)):\n",
    "                    hourly_meter.append(temp.meter_value[0])\n",
    "            \n",
    "        else:\n",
    "            #One data prior to the time going beyond a new hour hand is assumed to be the meter value when the new hour strikes.\n",
    "            hourly_meter.append(temp.meter_value[dataframe_row-1]) \n",
    "            \n",
    "            first_hour = first_hour + 1\n",
    "            \n",
    "            if (first_hour == 24):\n",
    "                first_hour = 0\n",
    "            \n",
    "            hourly_missing_data(dataframe_row, first_hour)\n",
    "\n",
    "    # The last hour of the ending point will be considered even if there is no data after that hh:00:00\n",
    "    if (t.dt.hour[len(t)-1] != end_datetime.hour):\n",
    "        if ((end_datetime.hour - t.dt.hour[len(t)-1]) < 0):\n",
    "            for i in range (0, 24 + (end_datetime.hour - t.dt.hour[len(t)-1])):\n",
    "                hourly_meter.append(temp.meter_value[len(t)-1])\n",
    "        else:\n",
    "            for i in range (0, end_datetime.hour - t.dt.hour[len(t)-1]):\n",
    "                hourly_meter.append(temp.meter_value[len(t)-1])\n",
    "           \n",
    "    if (t.dt.day[len(t)-1] != end_datetime.day):\n",
    "        for i in range (0, 24 * (end_datetime.day - t.dt.day[len(t)-1])):                            \n",
    "            hourly_meter.append(temp.meter_value[len(t)-1])\n",
    "\n",
    "            \n",
    "def hourly_missing_data(dataframe_row, first_hour):\n",
    "    if (t.dt.hour[dataframe_row] != first_hour):\n",
    "        print (\"There is missing data for an hour...................................................................\")\n",
    "        first_hour = first_hour + 1\n",
    "        hourly_meter.append(temp.meter_value[dataframe_row-1])\n",
    "    \n",
    "        if (first_hour == 24):\n",
    "            first_hour = 0\n",
    "    \n",
    "    # Repeat if there is missing data for more than an hour\n",
    "    if (t.dt.hour[dataframe_row] != first_hour):\n",
    "        return hourly_missing_data(dataframe_row, first_hour)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the meter_ID, starting time and ending time of the hourly data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Format: ID No.\n",
    "ID = 739\n",
    "\n",
    "#Format: 'yyyy-mm-dd hh:mm:ss'\n",
    "start_datetime = start_period('2015-10-01 06:00:00')\n",
    "\n",
    "#Format: 'yyyy-mm-dd hh:mm:ss'\n",
    "end_datetime = end_period('2015-11-01 06:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect hourly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['dataid'], sort=['localminute'])\n",
    "temp = grouped.get_group(ID).copy()\n",
    "temp['datetime'] = pd.to_datetime(temp['localminute'])\n",
    "temp = temp.loc[(temp['datetime'] >= start_datetime) & (temp['datetime'] <= end_datetime)]\n",
    "t = pd.to_datetime(temp['datetime'])\n",
    "temp.index = range(temp.shape[0])\n",
    "t.index = range(t.shape[0])\n",
    "\n",
    "\n",
    "hourly_reading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the hourly reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical limit of the graph is gauged by the lowest cumulative value of the gas consumption to the highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plot has vertical range from 88858 to 89544\n"
     ]
    }
   ],
   "source": [
    "ylim_low = hourly_meter[0]\n",
    "ylim_high = hourly_meter[len(hourly_meter)-1]\n",
    "print (\"The plot has vertical range from\", ylim_low, \"to\", ylim_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x25100e245c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(len(hourly_meter))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "ax.plot(x, hourly_meter, 'ok', ms=1)\n",
    "ax.set_xlim(0, len(hourly_meter) + 5)\n",
    "ax.set_ylim(ylim_low - 10, ylim_high + 10)\n",
    "ax.set_title('Generative model')\n",
    "plt.xlabel (\"Hours\")\n",
    "plt.ylabel (\"Meter value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 Find for each home, 5 houses with highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to varify if it works before I can run it\n",
    "\n",
    "\n",
    "#This should arrange them by increasing ID value & in increasing timeframe\n",
    "# houses = df.groupby(['dataid'], sort=['localminute'])\n",
    "df[\"localminute\"] = pd.to_datetime(df[\"localminute\"])                       #This gives in datetime64 [ns] format  \n",
    "df[\"localminute\"] = (df[\"localminute\"] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')   #This then converts them to int64  \n",
    "\n",
    "houseDT3 = df.groupby(['dataid'], sort=['localminute']).get_group(35).copy()\n",
    "\n",
    "print(houseDT3.localminute)\n",
    "\n",
    "\n",
    "# houseDT = df.groupby(['dataid'], sort=['localminute']).get_group(35).copy()\n",
    "# houseDT2 = df.groupby(['dataid'], sort=['localminute']).get_group(739).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Anh found this juicy piece of codes online @ \n",
    "#https://stackoverflow.com/questions/47661043/finding-correlation-coefficient-from-2-lists\n",
    "def mean(someList):\n",
    "    total = 0\n",
    "    for a in someList:\n",
    "        total += float(a)\n",
    "    mean = total/len(someList)\n",
    "    return mean\n",
    "def standDev(someList):\n",
    "    listMean = mean(someList)\n",
    "    dev = 0.0\n",
    "    for i in range(len(someList)):\n",
    "        dev += (someList[i]-listMean)**2\n",
    "    dev = dev**(1/2.0)\n",
    "    return dev\n",
    "def correlCo(someList1, someList2):\n",
    "\n",
    "    # First establish the means and standard deviations for both lists.\n",
    "    xMean = mean(someList1)\n",
    "    yMean = mean(someList2)\n",
    "    xStandDev = standDev(someList1)\n",
    "    yStandDev = standDev(someList2)\n",
    "    # r numerator\n",
    "    rNum = 0.0\n",
    "    for i in range(len(someList1)):\n",
    "        rNum += (someList1[i]-xMean)*(someList2[i]-yMean)\n",
    "\n",
    "    # r denominator\n",
    "    rDen = xStandDev * yStandDev\n",
    "\n",
    "    r =  rNum/rDen\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.corrcoef(list1, list2)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of meter id\n",
    "idList = df['dataid'].unique()\n",
    "\n",
    "#get a dataframe of data by hour:\n",
    "df_byhour = pd.DataFrame(df.groupby([df[\"localminute\"].dt.date.rename(\"Date\"), \n",
    "                               df[\"localminute\"].dt.hour.rename(\"Hour\"), \n",
    "                               df[\"dataid\"]]).mean().reset_index())\n",
    "\n",
    "#add a column called \"DateHour\" in the dataframe:\n",
    "df_byhour['DateHour'] = pd.to_datetime(df_byhour['Date'].apply(str)+' '+df_byhour['Hour'].apply(str)+':00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>dataid</th>\n",
       "      <th>meter_value</th>\n",
       "      <th>DateHour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>93470.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>94</td>\n",
       "      <td>116642.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>114</td>\n",
       "      <td>128294.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>187</td>\n",
       "      <td>263272.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>222</td>\n",
       "      <td>612262.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>252</td>\n",
       "      <td>329214.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>370</td>\n",
       "      <td>87880.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>483</td>\n",
       "      <td>360456.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>484</td>\n",
       "      <td>99298.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>739</td>\n",
       "      <td>88858.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>871</td>\n",
       "      <td>106464.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1086</td>\n",
       "      <td>83334.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1185</td>\n",
       "      <td>139336.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1283</td>\n",
       "      <td>167026.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1507</td>\n",
       "      <td>390354.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1556</td>\n",
       "      <td>202688.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1589</td>\n",
       "      <td>193922.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1697</td>\n",
       "      <td>266172.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1714</td>\n",
       "      <td>147048.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1718</td>\n",
       "      <td>161076.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1791</td>\n",
       "      <td>130806.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1792</td>\n",
       "      <td>135290.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1801</td>\n",
       "      <td>110108.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "      <td>204482.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2034</td>\n",
       "      <td>72376.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2335</td>\n",
       "      <td>202076.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2449</td>\n",
       "      <td>171120.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>2638</td>\n",
       "      <td>171832.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>3039</td>\n",
       "      <td>133640.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>3367</td>\n",
       "      <td>311094.0</td>\n",
       "      <td>2015-10-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397617</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5129</td>\n",
       "      <td>166488.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397618</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5131</td>\n",
       "      <td>121772.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397619</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5275</td>\n",
       "      <td>174894.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397620</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5403</td>\n",
       "      <td>125074.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397621</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5484</td>\n",
       "      <td>285692.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397622</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5785</td>\n",
       "      <td>188190.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397623</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5810</td>\n",
       "      <td>115272.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397624</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5814</td>\n",
       "      <td>815822.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397625</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>5972</td>\n",
       "      <td>318824.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397626</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>6412</td>\n",
       "      <td>156218.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397627</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>6673</td>\n",
       "      <td>85362.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397628</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>6830</td>\n",
       "      <td>166072.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397629</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>6910</td>\n",
       "      <td>205224.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397630</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7016</td>\n",
       "      <td>313488.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397631</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7017</td>\n",
       "      <td>409532.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397632</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7030</td>\n",
       "      <td>160704.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397633</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7117</td>\n",
       "      <td>230136.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397634</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7287</td>\n",
       "      <td>239084.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397635</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7429</td>\n",
       "      <td>149392.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397636</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7674</td>\n",
       "      <td>292212.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397637</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7682</td>\n",
       "      <td>248624.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397638</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>7989</td>\n",
       "      <td>124868.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397639</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>8156</td>\n",
       "      <td>270292.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397640</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>8829</td>\n",
       "      <td>175860.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397641</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>8890</td>\n",
       "      <td>227326.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397642</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>8967</td>\n",
       "      <td>189188.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397643</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9295</td>\n",
       "      <td>183664.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397644</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9631</td>\n",
       "      <td>124500.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397645</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9729</td>\n",
       "      <td>138146.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397646</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>9766</td>\n",
       "      <td>179902.0</td>\n",
       "      <td>2016-04-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397647 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date  Hour  dataid  meter_value            DateHour\n",
       "0       2015-10-01     5      35      93470.0 2015-10-01 05:00:00\n",
       "1       2015-10-01     5      94     116642.0 2015-10-01 05:00:00\n",
       "2       2015-10-01     5     114     128294.0 2015-10-01 05:00:00\n",
       "3       2015-10-01     5     187     263272.0 2015-10-01 05:00:00\n",
       "4       2015-10-01     5     222     612262.0 2015-10-01 05:00:00\n",
       "5       2015-10-01     5     252     329214.0 2015-10-01 05:00:00\n",
       "6       2015-10-01     5     370      87880.0 2015-10-01 05:00:00\n",
       "7       2015-10-01     5     483     360456.0 2015-10-01 05:00:00\n",
       "8       2015-10-01     5     484      99298.0 2015-10-01 05:00:00\n",
       "9       2015-10-01     5     739      88858.0 2015-10-01 05:00:00\n",
       "10      2015-10-01     5     871     106464.0 2015-10-01 05:00:00\n",
       "11      2015-10-01     5    1086      83334.0 2015-10-01 05:00:00\n",
       "12      2015-10-01     5    1185     139336.0 2015-10-01 05:00:00\n",
       "13      2015-10-01     5    1283     167026.0 2015-10-01 05:00:00\n",
       "14      2015-10-01     5    1507     390354.0 2015-10-01 05:00:00\n",
       "15      2015-10-01     5    1556     202688.0 2015-10-01 05:00:00\n",
       "16      2015-10-01     5    1589     193922.0 2015-10-01 05:00:00\n",
       "17      2015-10-01     5    1697     266172.0 2015-10-01 05:00:00\n",
       "18      2015-10-01     5    1714     147048.0 2015-10-01 05:00:00\n",
       "19      2015-10-01     5    1718     161076.0 2015-10-01 05:00:00\n",
       "20      2015-10-01     5    1791     130806.0 2015-10-01 05:00:00\n",
       "21      2015-10-01     5    1792     135290.0 2015-10-01 05:00:00\n",
       "22      2015-10-01     5    1801     110108.0 2015-10-01 05:00:00\n",
       "23      2015-10-01     5    2018     204482.0 2015-10-01 05:00:00\n",
       "24      2015-10-01     5    2034      72376.0 2015-10-01 05:00:00\n",
       "25      2015-10-01     5    2335     202076.0 2015-10-01 05:00:00\n",
       "26      2015-10-01     5    2449     171120.0 2015-10-01 05:00:00\n",
       "27      2015-10-01     5    2638     171832.0 2015-10-01 05:00:00\n",
       "28      2015-10-01     5    3039     133640.0 2015-10-01 05:00:00\n",
       "29      2015-10-01     5    3367     311094.0 2015-10-01 05:00:00\n",
       "...            ...   ...     ...          ...                 ...\n",
       "397617  2016-04-01     4    5129     166488.0 2016-04-01 04:00:00\n",
       "397618  2016-04-01     4    5131     121772.0 2016-04-01 04:00:00\n",
       "397619  2016-04-01     4    5275     174894.0 2016-04-01 04:00:00\n",
       "397620  2016-04-01     4    5403     125074.0 2016-04-01 04:00:00\n",
       "397621  2016-04-01     4    5484     285692.0 2016-04-01 04:00:00\n",
       "397622  2016-04-01     4    5785     188190.0 2016-04-01 04:00:00\n",
       "397623  2016-04-01     4    5810     115272.0 2016-04-01 04:00:00\n",
       "397624  2016-04-01     4    5814     815822.0 2016-04-01 04:00:00\n",
       "397625  2016-04-01     4    5972     318824.0 2016-04-01 04:00:00\n",
       "397626  2016-04-01     4    6412     156218.0 2016-04-01 04:00:00\n",
       "397627  2016-04-01     4    6673      85362.0 2016-04-01 04:00:00\n",
       "397628  2016-04-01     4    6830     166072.0 2016-04-01 04:00:00\n",
       "397629  2016-04-01     4    6910     205224.0 2016-04-01 04:00:00\n",
       "397630  2016-04-01     4    7016     313488.0 2016-04-01 04:00:00\n",
       "397631  2016-04-01     4    7017     409532.0 2016-04-01 04:00:00\n",
       "397632  2016-04-01     4    7030     160704.0 2016-04-01 04:00:00\n",
       "397633  2016-04-01     4    7117     230136.0 2016-04-01 04:00:00\n",
       "397634  2016-04-01     4    7287     239084.0 2016-04-01 04:00:00\n",
       "397635  2016-04-01     4    7429     149392.0 2016-04-01 04:00:00\n",
       "397636  2016-04-01     4    7674     292212.0 2016-04-01 04:00:00\n",
       "397637  2016-04-01     4    7682     248624.0 2016-04-01 04:00:00\n",
       "397638  2016-04-01     4    7989     124868.0 2016-04-01 04:00:00\n",
       "397639  2016-04-01     4    8156     270292.0 2016-04-01 04:00:00\n",
       "397640  2016-04-01     4    8829     175860.0 2016-04-01 04:00:00\n",
       "397641  2016-04-01     4    8890     227326.0 2016-04-01 04:00:00\n",
       "397642  2016-04-01     4    8967     189188.0 2016-04-01 04:00:00\n",
       "397643  2016-04-01     4    9295     183664.0 2016-04-01 04:00:00\n",
       "397644  2016-04-01     4    9631     124500.0 2016-04-01 04:00:00\n",
       "397645  2016-04-01     4    9729     138146.0 2016-04-01 04:00:00\n",
       "397646  2016-04-01     4    9766     179902.0 2016-04-01 04:00:00\n",
       "\n",
       "[397647 rows x 5 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_byhour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 183 days => 4392 hours => 4392 readings in each list\n",
    "hourlyDataDict = {}\n",
    "\n",
    "prevHr = pd.Timestamp('2015-10-01 05:00:00')\n",
    "endHr = pd.Timestamp('2016-04-01 04:00:00')\n",
    "for id1 in idList:\n",
    "    dfTemp = df_byhour.groupby(['dataid']).get_group(id1)\n",
    "    hourList = dfTemp['DateHour'].tolist()\n",
    "    valList = dfTemp['meter_value'].tolist()\n",
    "    \n",
    "    prevVal = valList[0]\n",
    "    newList = [valList[0]]\n",
    "    for index, hr in enumerate(hourList):\n",
    "        #if hour is more than an hour bigger than previous\n",
    "        while (hr - prevHr).seconds >= 3600:\n",
    "            newList.append(prevVal)\n",
    "            #increment every hour\n",
    "            prevHr += pd.Timedelta(seconds=3600)\n",
    "            #print(\"append in loop\" + \" hour is: \" + str(prevHr))\n",
    "    \n",
    "\n",
    "        prevVal = valList[index]\n",
    "    \n",
    "    while prevHr < endHr:\n",
    "        newList.append(prevVal)\n",
    "        #increment every hour\n",
    "        prevHr += pd.Timedelta(seconds=3600)\n",
    "    \n",
    "    hourlyDataDict[id1] = newList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "print(hourlyDataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
