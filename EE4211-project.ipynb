{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE4211 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Team 1: Quadratic                   Done by Choi Jae Hyung, Lee Min Young, Nicholas Lui Ming Yang, Tran Duy Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"project_data.csv\")\n",
    "# df.shape - (1584823, 3)\n",
    "# df column values:\n",
    "# localminute - Timestamp \n",
    "# dataid - MeterID \n",
    "# meter_value - meter reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many houses are included in the measurement study?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since dataid is unique for each meter, we can count the number of unique dataid numbers\n",
    "df.dataid.value_counts().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This gives us 157 for the number of houses included in the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are there any malfunctioning meters? If so, identify them and the time periods where they were malfunctioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are various ways to define a malfunctioning meter. Let us explore some of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us first check, if there are *meters that report a lower value at a later timestamp*. Since meter_value is cumulative consumption, meter_value should not be lower at a later timestamp for the same meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['dataid'], sort=['localminute'])\n",
    "def has_decreasing_values(df):\n",
    "    current_value = 0\n",
    "    for index, val in df.iteritems():\n",
    "        if val < current_value:\n",
    "            return True\n",
    "        current_value = val\n",
    "        \n",
    "meters_with_decreasing = (grouped['meter_value']\n",
    "                          .agg(has_decreasing_values)\n",
    "                          .where(lambda x: x)\n",
    "                          .dropna()\n",
    "                          .keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([  35,   77,   94,  483,  484, 1042, 1086, 1185, 1507, 1556, 1718,\n",
       "            1790, 1801, 2129, 2335, 2449, 3134, 3527, 3544, 3893, 4031, 4193,\n",
       "            4514, 4998, 5129, 5131, 5193, 5403, 5810, 5814, 5892, 6836, 7017,\n",
       "            7030, 7117, 7739, 7794, 7989, 8156, 8890, 9134, 9639, 9982],\n",
       "           dtype='int64', name='dataid')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(meters_with_decreasing))\n",
    "meters_with_decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wow, we have 43 meters that have a decreasing value! Let's zoom in and get the offending sections for each meter. (In other words, find the exact data points which show this descreasing value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0c2225967d6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mgroup_rows_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_rows_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meter_value'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcurrent_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0moffending_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mcurrent_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'meter_value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2102\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2104\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2106\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_loc\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   2624\u001b[0m                                                       \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2625\u001b[0m                                                       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2626\u001b[1;33m                                                       dtype=new_values.dtype)\n\u001b[0m\u001b[0;32m   2627\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2628\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    273\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[1;32m--> 275\u001b[1;33m                                        raise_cast_failure=True)\n\u001b[0m\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[0;32m   4084\u001b[0m                     \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4085\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4086\u001b[1;33m                 \u001b[0msubarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4087\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4088\u001b[0m             \u001b[1;31m# don't coerce Index types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_try_cast\u001b[1;34m(arr, take_fast_path)\u001b[0m\n\u001b[0;32m   4045\u001b[0m             \u001b[1;31m# Take care in creating object arrays (but iterators are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4046\u001b[0m             \u001b[1;31m# supported):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4047\u001b[1;33m             if is_object_dtype(dtype) and (is_list_like(subarr) and\n\u001b[0m\u001b[0;32m   4048\u001b[0m                                            not (is_iterator(subarr) or\n\u001b[0;32m   4049\u001b[0m                                            isinstance(subarr, np.ndarray))):\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\dtypes\\inference.py\u001b[0m in \u001b[0;36mis_list_like\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     return (isinstance(obj, Iterable) and\n\u001b[1;32m--> 284\u001b[1;33m             not isinstance(obj, string_and_binary_types))\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Need to re-sort after filter since filter takes all rows and breaks original sorting\n",
    "dec_meters = (grouped.filter(lambda x: int(x.name) in meters_with_decreasing)\n",
    "              .groupby(['dataid'], sort=['localminute']))\n",
    "\n",
    "## Iterate over values to find offending rows for each meter\n",
    "## WARNING: RUNS VERY SLOWLY. TODO: OPTIMIZE\n",
    "offending_values = {}\n",
    "for group_id, rows in dec_meters.groups.items():\n",
    "    offending_values[group_id] = []\n",
    "    current_value = 0\n",
    "    group_rows = dec_meters.get_group(group_id)\n",
    "    group_rows_count = group_rows.shape[0]\n",
    "    for i in range(group_rows_count):\n",
    "        if group_rows.iloc[i]['meter_value'] < current_value:\n",
    "            offending_values[group_id].append([group_rows.iloc[i-1], group_rows.iloc[i]])\n",
    "        current_value = group_rows.iloc[i]['meter_value']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of 'broken data' instances for each meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meter ID |\", \"Number of broken readings\")\n",
    "for k, v in offending_values.items():\n",
    "    print(str(k).ljust(20), len(v))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just knowing the number of broken readings may not be useful. \n",
    "##### Let's say that we want to know which meters should be fixed, since faulty meters result in us inaccurately measuring consumption, and possibly losing money.\n",
    "##### There should be some measure of tolerance in deciding if a meter is broken. In this case, let's check the average decreasing amount, and the ratio of \"broken\" readings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Meter ID |\", \"Number of broken readings |\", \"Average decrease across broken readings |\", \"Percentage of broken readings for meter\")\n",
    "for k, v in offending_values.items():\n",
    "    print(str(k).ljust(20), \n",
    "          str(len(v)).ljust(20), \n",
    "          str(reduce(lambda x, y: x + abs(y[1]['meter_value'] - y[0]['meter_value']) / len(v), [0] + v )).ljust(40), \n",
    "          (len(v) / dec_meters.get_group(k).shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the data laid out like this, it is pretty clear that most meters are not really broken if we allow a 2% error rate\n",
    "#### However, in terms of error volume, some are pretty suspect. Let's use an average volume error of 100 Cubic foot (that's a lot!) as our threshold, and filter our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Meter ID |\", \"Number of broken readings |\", \"Average decrease across broken readings |\", \"Percentage of broken readings for meter\")\n",
    "for k, v in offending_values.items():\n",
    "    measure = str(reduce(lambda x, y: x + abs(y[1]['meter_value'] - y[0]['meter_value']) / len(v), [0] + v )).ljust(40)\n",
    "    if float(measure) > 10:\n",
    "        print(str(k).ljust(20), \n",
    "              str(len(v)).ljust(20), \n",
    "              measure, \n",
    "              (len(v) / dec_meters.get_group(k).shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use this function to validate/check each bad meter given our heuristic\"\"\"\n",
    "def print_bad_meter_readings(meterID):\n",
    "    meter_readings = offending_values[meterID]\n",
    "    print(\"time apart |\".ljust(20), \"meter value initial |\", \"meter value after |\", \"difference\")\n",
    "    for readings_pair in meter_readings:\n",
    "\n",
    "        print(str(pd.to_datetime(readings_pair[1]['localminute']) - pd.to_datetime(readings_pair[0]['localminute'])).ljust(25), \n",
    "              str(readings_pair[0]['meter_value']).ljust(20), \n",
    "              str(readings_pair[1]['meter_value']).ljust(20), \n",
    "              readings_pair[1]['meter_value'] - readings_pair[0]['meter_value'])\n",
    "print_bad_meter_readings(1556)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another requirement of the meters is that they push their readings to be saved if the meter values differ by at least 2 cubic foot\n",
    "#### Let us verify that every pair of readings for a meter differ by at least 2 cubic foot. To not double count, we will only check for readings where the differing value is 0 >= x > 2 so that we don't get the same error readings where the readings decrease\n",
    "#### Since the value is smaller, we will not focus on the difference in values, but on the percentage of total readings for the meter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2 = df.groupby(['dataid'], sort=['localminute'])\n",
    "def has_stagnant_values(df):\n",
    "    current_value = 0\n",
    "    for index, val in df.iteritems():\n",
    "        if index == 0:\n",
    "            current_value = val\n",
    "            continue\n",
    "        \n",
    "        if val < current_value + 2 and val >= current_value:\n",
    "            return True\n",
    "        current_value = val\n",
    "        \n",
    "meters_with_stagnant = (grouped['meter_value']\n",
    "                          .agg(has_stagnant_values)\n",
    "                          .where(lambda x: x)\n",
    "                          .dropna()\n",
    "                          .keys())\n",
    "\n",
    "print(\"Number of meters with stagnant values: \", len(meters_with_stagnant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following segment should have calculated the percentage of stagnant values for each meter, but it takes too long (tried for 5 minutes and gave up). At least we know number of meters that reported stagnant values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Need to re-sort after filter since filter takes all rows and breaks original sorting\n",
    "# stagnant_meters = (grouped2.filter(lambda x: int(x.name) in meters_with_stagnant)\n",
    "#               .groupby(['dataid'], sort=['localminute']))\n",
    "\n",
    "# ## Iterate over values to count offending occurences. Not counting value\n",
    "# ## WARNING: RUNS VERY SLOWLY. TODO: OPTIMIZE\n",
    "# offending_values2 = {}\n",
    "# for group_id, rows in stagnant_meters.groups.items():\n",
    "#     offending_values2[group_id] = 0\n",
    "#     group_rows = stagnant_meters.get_group(group_id)\n",
    "#     group_rows_count = group_rows.shape[0]\n",
    "#     # Set current_value so we do not trigger first reading\n",
    "#     current_value = group_rows.iloc[0]['meter_value'] - 5\n",
    "#     for i in range(group_rows_count):\n",
    "#         if group_rows.iloc[i]['meter_value'] < current_value + 2 and group_rows.iloc[i]['meter_value'] >= current_value:\n",
    "#             offending_values2[group_id] += 1\n",
    "#         current_value = group_rows.iloc[i]['meter_value']\n",
    "\n",
    "        \n",
    "# print(\"Meter ID |\", \"Number of broken readings |\", \"Percentage of broken readings for meter\")\n",
    "# for k, v in offending_values2.items():\n",
    "#     print(str(k).ljust(20), \n",
    "#           str(v).ljust(20),  \n",
    "#           (v / stagnant_meters.get_group(k).shape[0]) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagnant_meters = (grouped2.filter(lambda x: int(x.name) in meters_with_stagnant)\n",
    "              .groupby(['dataid'], sort=['localminute']))\n",
    "\n",
    "print(\"Total number of readings to iterate, which is why it is slow: \", \n",
    "      reduce(lambda x, y: x + len(y), [0] + list(stagnant_meters.groups.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now have two different criterion for deciding what constitues a broken meter, and the readings that helped us determine that. Since the question asks for \"time periods where they were malfunctioning.\", let's demonstrate that we can consolidate broken readings into time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the meter with the most broken readings (based off decreasing values)\n",
    "most_broken_values_meter = grouped.get_group(5403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using a stricter requirement now. Let's consider that, if the subsequent meter reading is less \n",
    "# than an increment of 2 (meters are only supposed to report after at least an increment of 2)\n",
    "\n",
    "# Broken_criteria is a helper function (comparator function) that takes in two (in-order) readings and outputs a \n",
    "# boolean of whether the later reading is \"broken\"\n",
    "\n",
    "broken_criteria = lambda x, y: y['meter_value'] < x['meter_value'] + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_readings = 0\n",
    "num_readings = most_broken_values_meter.shape[0]\n",
    "for i in range(1, num_readings):\n",
    "    if broken_criteria(most_broken_values_meter.iloc[i-1], most_broken_values_meter.iloc[i]):\n",
    "        broken_readings += 1\n",
    "\n",
    "print(\"Number of broken readings for meter 5403, based on stricter requirements: \", broken_readings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now create a function that can aggregate a meter's readings into \"broken\" time periods\n",
    "# Return value of the function will be as follow:\n",
    "\"\"\"\n",
    "2D array. \n",
    "First dimension is the time periods. \n",
    "Second dimension is the consecutive broken readings that make up the broken time period\n",
    "To find the actual time data, take the ['localminute'] attribute from the first and last reading per period\n",
    "    time_periods = [\n",
    "        [reading_1, reading_2, reading_3],\n",
    "        [reading_1, reading_2]\n",
    "    ]\n",
    "\"\"\"\n",
    "def get_broken_time_periods(broken_criteria, meter_readings):\n",
    "    num_readings = meter_readings.shape[0]\n",
    "    time_periods = []\n",
    "    temp_period = []\n",
    "    for i in range(1, num_readings):\n",
    "        if broken_criteria(meter_readings.iloc[i-1], meter_readings.iloc[i]):\n",
    "            temp_period.append(meter_readings.iloc[i])\n",
    "        else:\n",
    "            if temp_period:\n",
    "                time_periods.append(temp_period)\n",
    "                temp_period = []\n",
    "    if temp_period:\n",
    "        time_periods.append(temp_period)\n",
    "    return time_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_time_periods = get_broken_time_periods(broken_criteria, most_broken_values_meter)\n",
    "print(\"Number of broken time periods: \", len(broken_time_periods))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.2 \n",
    "## Hourly Data Collection and Plotting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: \n",
    "Obtain hourly data from a list of given data, and plot the obtained data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters: \n",
    "#### 1. Gas meter ID \n",
    "Gas meter with ID 739 will be focused for the study\n",
    "#### 2. Starting time\n",
    "6th Oct 2015, 12 a.m.\n",
    "#### 3. Ending time\n",
    "5th Nov 2015, 11 p.m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumption (How we aggregate the data into hourly readings)\n",
    "- The data is always cumulative: increasing in value as time goes.\n",
    "\n",
    "The lowest value in an hour should be the closest value to an exact hour (hh:00:00) and a good estimate of that hour's data.\n",
    "\n",
    "Therefore, the first data point in an hour is used to represent the hourly data. (The assumption is that we are interested in approximating the consumption amount at the start of the hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to handle missing data?\n",
    "As the data is cumulative gas consumption by household, when there is no data, it can be assumed as there is insignificant gas consumption between that period.\n",
    "\n",
    "Hence, the missing data will be given same value as most recent hourly data obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad data readings\n",
    "This plot of hourly readings does not account for \"bad\" data readings (consecutive reading with a lower value, which should not happen for cumulative consumption data)\n",
    "\n",
    "The likelihood of seeing an hourly data point that is bad is Average P(bad readings an hour) * (4*60), since we have max 4 readings per minute\n",
    "\n",
    "This reduces the code complexity to get a plot, and the usefulness of this approach depends on the usage of this plot\n",
    "\n",
    "If we are simply looking for a general consumption level visualisation, then this plot should suffice. However, if we are looking for plots with a decrease in value to identify faulty meters, then this plot is insufficient. For that, we cannot hold our original assumption (that readings are cumulative and strictly increasing), and should take the MIN(hour_values) to represent that hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm of hourly data collection consists of the functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In every new hour, collect the lowest value as the hourly data.\n",
    "\n",
    "\n",
    "2. Look out for missing hour. When missing hour is found, previuos hour data is given for that hour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DateHour column, that represents \"Date + Hour\" of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to datetime format\n",
    "df[\"localminute\"] = pd.to_datetime(df[\"localminute\"])\n",
    "#get a dataframe of data by hour:\n",
    "df_byhour = pd.DataFrame(df.groupby([df[\"localminute\"].dt.date.rename(\"Date\"), \n",
    "                               df[\"localminute\"].dt.hour.rename(\"Hour\"), \n",
    "                               df[\"dataid\"]]).min().reset_index())\n",
    "\n",
    "# To preview df_byhour sorted by meter and time\n",
    "# df_byhour = df_byhour.sort_values=(by=['dataid', 'Date', 'Hour'])\n",
    "\n",
    "#add a column called \"DateHour\" in the dataframe:\n",
    "df_byhour['DateHour'] = pd.to_datetime(df_byhour['Date'].apply(str)+' '+df_byhour['Hour'].apply(str)+':00:00')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the meter_ID, starting time and ending time of the hourly data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format: ID No.\n",
    "ID = 739\n",
    "\n",
    "#ranges from 06/10/2015 00:00:00 to 05/11/2015 23:00:00\n",
    "start_date = datetime.date(2015, 10, 5)\n",
    "end_date = datetime.date(2015, 11, 6)\n",
    "filteredDF = df_byhour[(df_byhour[\"Date\"] > start_date) & (df_byhour[\"Date\"] < end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect hourly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 06/10/2015 -> 06/11/2015 there are 31 days -> 744 data points\n",
    "prevHr = pd.Timestamp('2015-10-06 00:00:00')\n",
    "endHr = pd.Timestamp('2015-11-05 23:00:00')\n",
    "#generate a temporary dataframe through get_group function with each id\n",
    "dfTemp = filteredDF.groupby(['dataid']).get_group(ID)\n",
    "#get the list of hours and meter values\n",
    "hourList = dfTemp['DateHour'].tolist()\n",
    "valList = dfTemp['meter_value'].tolist()\n",
    "#re-initialize the prev val as the first value of the val list\n",
    "prevVal = valList[0]\n",
    "#re-initialize the list of values as a list with 1 item\n",
    "newList = [valList[0]]\n",
    "#loop through each hour in the list\n",
    "for index, hr in enumerate(hourList):\n",
    "    #if hour is more than an hour bigger than previous\n",
    "    while (hr - prevHr).seconds >= 3600:\n",
    "        newList.append(prevVal)\n",
    "        #increment every hour\n",
    "        prevHr += pd.Timedelta(seconds=3600)\n",
    "        \n",
    "    #update the prev value everytime one index is passed\n",
    "    prevVal = valList[index]\n",
    "    \n",
    "#some data does not reach until the end date, thats where this comes in\n",
    "while prevHr < endHr:\n",
    "    newList.append(prevVal)\n",
    "    #increment every hour\n",
    "    prevHr += pd.Timedelta(seconds=3600)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the hourly reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.date_range(datetime.date(2015, 10, 6), datetime.date(2015, 11, 6), freq = 'H').tolist()\n",
    "#get rid of the last hour which is 00:00:00 06/11/2016\n",
    "x.pop()\n",
    "y = newList\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n",
    "ax.plot(x, y, 'ok', ms=1)\n",
    "plt.xlabel (\"Time\")\n",
    "plt.ylabel (\"Meter value\")\n",
    "ax.set_title('Hourly Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.3 Find for each home, 5 houses with highest correlation\n",
    "\n",
    "#### Uses Numpy.corrcoeff to get Pearson product-moment correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of meter id\n",
    "idList = df['dataid'].unique()\n",
    "\n",
    "df[\"localminute\"] = pd.to_datetime(df[\"localminute\"])\n",
    "#get a dataframe of data by hour:\n",
    "df_byhour = pd.DataFrame(df.groupby([df[\"localminute\"].dt.date.rename(\"Date\"), \n",
    "                               df[\"localminute\"].dt.hour.rename(\"Hour\"), \n",
    "                               df[\"dataid\"]]).mean().reset_index())\n",
    "\n",
    "#add a column called \"DateHour\" in the dataframe:\n",
    "df_byhour['DateHour'] = pd.to_datetime(df_byhour['Date'].apply(str)+' '+df_byhour['Hour'].apply(str)+':00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 183 days => 4392 hours => 4392 readings in each list\n",
    "#generate a dictionary\n",
    "hourlyDataDict = {}\n",
    "\n",
    "#define start hour and end hour\n",
    "staHr = pd.Timestamp('2015-10-01 05:00:00')\n",
    "endHr = pd.Timestamp('2016-04-01 04:00:00')\n",
    "\n",
    "#loop through each id \n",
    "for id1 in idList:\n",
    "    \n",
    "    #re-innitialize the prev Hour as start hour\n",
    "    prevHr = staHr\n",
    "    #generate a temporary dataframe through get_group function with each id\n",
    "    dfTemp = df_byhour.groupby(['dataid']).get_group(id1)\n",
    "    #get the list of hours and meter values\n",
    "    hourList = dfTemp['DateHour'].tolist()\n",
    "    valList = dfTemp['meter_value'].tolist()\n",
    "    #re-initialize the prev pal as the first value of the val list\n",
    "    prevVal = valList[0]\n",
    "    #re-initialize the list of values as a list with 1 item\n",
    "    newList = [valList[0]]\n",
    "    #loop through each hour in the list\n",
    "    for index, hr in enumerate(hourList):\n",
    "        #if hour is more than an hour bigger than previous\n",
    "        while (hr - prevHr).seconds >= 3600:\n",
    "            newList.append(prevVal)\n",
    "            #increment every hour\n",
    "            prevHr += pd.Timedelta(seconds=3600)\n",
    "        \n",
    "        #update the prev value everytime one index is passed\n",
    "        prevVal = valList[index]\n",
    "    \n",
    "    #some data does not reach until the end date, thats where this comes in\n",
    "    while prevHr < endHr:\n",
    "        newList.append(prevVal)\n",
    "        #increment every hour\n",
    "        prevHr += pd.Timedelta(seconds=3600)\n",
    "    \n",
    "    #after all that create a new entry in the Dictionary with the key as the id and value is the list of meter readings\n",
    "    hourlyDataDict[id1] = newList\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Dictionary of Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corDict = {}\n",
    "#loop through id List to get the first id\n",
    "for id1 in idList:\n",
    "    #create the empty value for the first key\n",
    "    corDict[id1] = {}\n",
    "    #loop through the id list to get the second id\n",
    "    for id2 in idList:\n",
    "        # generate the coefficient through numpy.corrcoef (can be changed later)\n",
    "        coef = np.corrcoef(hourlyDataDict[id1], hourlyDataDict[id2])[0, 1]\n",
    "        # assign the value to the dict with the appropriate key\n",
    "        corDict[id1][id2] = coef\n",
    "        #print(\"coefficient between \" + str(id1) + \" and \" + str(id2) + \" is: \" + str(coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's find the top 5 houses with highest correlation value for each house "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "idList.sort()\n",
    "top5dict = {}\n",
    "\n",
    "for id in idList:\n",
    "    \n",
    "    #### This is if you just want the houseID for each (first column being the ID)\n",
    "    \n",
    "#     print(sorted(corDict[id], key =corDict[id].get, reverse=True)[:6])\n",
    "    \n",
    "    #### This is if you want to get the houseID + the value of correlation coefficient\n",
    "    c = Counter(corDict[id])\n",
    "    mc = c.most_common(6)\n",
    "    del mc[0]\n",
    "    \n",
    "    top5dict.update({id : mc})\n",
    "    \n",
    "top5dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 In this part, you will asked to build a model to forecast the hourly readings in the future (next hour). Can you explain why you may want to forecast the gas consumption in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting gas consumption in the future is necessary because of many reasons:\n",
    "1. Gas is a natural resource and it is limited, predicting gas consumption will shed some light onto how much more we need to produce and when it is necessary to cut down on the consumption for fear of exhausting this natural resource.\n",
    "2. Future gas consumption will also help people manage their finances better\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who would find this information valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to list down a few: customers, government, environmental activists, gas company, maintenance company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What can you do if you have a good forecasting model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sell it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build a linear regression model to forecast the hourly readings in the future (next hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import machine learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfDict = {}\n",
    "for key in hourlyDataDict:\n",
    "    #each key is a linear regression model\n",
    "    clfDict[key] =  LR()\n",
    "    #features_train = the number of hour from 5:00:00 01/10/2015\n",
    "    features_train = np.asarray(range(4392)).reshape(-1,1)\n",
    "    \n",
    "    #print(features_train)\n",
    "    #labels_train = values\n",
    "    labels_train = np.asarray(hourlyDataDict[key])\n",
    "    \n",
    "    #fit the model accordingly\n",
    "    clfDict[key].fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you want to do linear prediction use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100213.91220764])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#: clfDict['meter_id']([['numberofhours']])\n",
    "clfDict[739].predict([[3333]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
